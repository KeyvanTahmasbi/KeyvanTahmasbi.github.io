**Keyvan Tahmasbi (ktahmasbi77@gmail.com)**
# فصل 2: رگرسیون و توزیع نرمال
### توزیع نرمال دو متغیره


$$
\textbf{y} = \begin{pmatrix} x \\ y \end{pmatrix}
,
\mu = \begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix}
,
\Sigma =  \begin{pmatrix} \sigma^2_x & \sigma^2_{xy} \\ \sigma^2_{xy} & \sigma^2_y \end{pmatrix}
$$

پارامترهای توزیع کناری:

$E(x) = \mu_x$ , $E(y)=\mu_y$ , $var(x)=\sigma_x^2$ , $var(y) = \sigma_y^2$

همبستگی بین x, y:

$\rho = \frac{\sigma_{xy}}{\sigma_x \sigma_y}$

# 
### شبیه سازی  و برآورد پارامترهای مدل

مرحله اول: تولید نمونه تصادفی ب اساس مدل پیشنهادی

ابتدا متغیرهای تصادفی و پارامترها را در مدل پیشنهادی تعیین می کنیم. برای مثال مدل زیر را در نظر بگیرید:


$$ y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \varepsilon $$

بطوری که:

$$x_{1i} \sim N(0, 1)$$
$$x_{2i} \sim Ber(0.5)$$
$$ \varepsilon_i \sim N(0,0.5)$$
$$\beta_0 = 0 , \beta_1=1, \beta_2=-1, \sigma^2=0.5$$

 داریم:

$$y_i \sim N(\beta_0 + \beta_1 x_{1i}+\beta_2x_{2i}, \sigma^2)$$


متغیرهای تصادفی از توزیع های آماری مشخص تولید می کنیم (مثلا برای $_1x$ توزیع نرمال و برای $_2x$ توزیع برنولی در نظر میگیریم.) سپس به پارامتر های مدل ($_0\beta$ و $_1\beta$ و $_2\beta$ و $^2\sigma$) مقدار واقعی می دهیم.


برای $\varepsilon$ توزیع نرمال درنظر میگیریم ومقادیر را برای آن تولید می کنیم.

ماتریس واریانس و کوواریانس و بردار میانگین اعلام شده را نیز وارد میکنیم.

بر اساس آن مقادیر متغیرهای مستقل ($_1x$ و $_2x$) را تولید می کنیم.

سپس مدل را میسازیم.


## توزیع شرطی نرمال دو متغیره
### مثال
فرض کنید ماتریس پراکندگی متغیرها $_1y$ , $_2y$, $_3y$ به صورت زیر است:

$$
\begin{pmatrix} 1 & 0.5 & 0.5 \\ 0.5 & 1 & 0.25 \\ 0.5 & 0.25 & 1 \end{pmatrix}
$$

# فصل 3

## شناساپذیری و برآوردپذیری
می خواهیم شناساپذیری را چک کنیم. 

پارامتری که شناساپذیر باشد، برآوردپذیر است اما عکس آن ممکن است صادق نباشد. 

مثال:

$$f_{\theta_1,\theta_2}(x) = \frac{\theta_1 }{\theta_2} e^{\frac{\theta_1 }{\theta_2}x} $$

نکته: در مواردی که پارامتر ها تقسیم، تفریق  شده اند معمولا شناساپذیری نداریم.

ما نمیتوانیم براوردهای جداگانه ای برای $_1\theta$ و $_2\theta$ بدست آوریم. برای همین طبق قضیه زهنا می توانیم برآوردی برای $\frac{_1\theta}{_2\theta}$ بدست آوریم. 

شناساپذیری:

$$\theta_1 \neq \theta_2 \Leftrightarrow f_{\theta_1}(x) \neq f_{\theta_2}(x) $$
 
```R
# identifibility
theta1 = 1
theta2 = 1
lambda = theta1/theta2
X = rexp(100, lambda) 
hist(X)

# MLE
p = c(1,1)
L = rep(0,100)
f = function(p){
    for(i in 1:100){
        L[i] = dexp(X[i], p[1]/p[2])
    }
    Lfinal = sum(log(L))
    ;-Lfinal
}
f(p)
p = c(2,2)

```
وقتی p=c(1,1) برای برآورد تتا1 داریم 94.97792 و وقتی p = c(2,2 ) برای برآورد تتا2 نیز داریم 94.97792. پس شناساپذیر نیست. یعنی نمیتوانیم برآوردی برای تتا1 و تتا2 به طور جداگانه بدست آوریم. اگرچه که نرم افزار این کار را انجام می دهد ولی کاری غلط است. 

```R
M = nlminb(p, f ,lower=c(0,0, upper=c(Inf,Inf)))
M
library(nlme)
h = fdHess(M$par, f)
ih = solve(h$Hessian)
se = diag(sqrt(ih))
MLE = M$par
Results = cbind(p, MLE, se) 
> Results
     p      MLE       se
[1,] 2 2.076198 52.73225
[2,] 2 1.971930 50.08399
```
انحراف استاندارد (se) اگر خیلی بزرگ یا صفر باشد بیانگر شناساناپذیری پارامترها می باشد. 

برای اینکه بتوانیم پارامتر ها را شناساپذیر کنیم نیاز است که یکس از آنها را ثابت در نظر بگیریم 

```R
f = function(p){
    p[2] = 2
    for(i in 1:100){
        L[i] = dexp(X[i], p[1]/p[2])
    }
    Lfinal = sum(log(L))
    ;-Lfinal
}
f(p)
p = 2
M = nlminb(p, f ,lower=0 , upper=Inf)
M
library(nlme)
h = fdHess(M$par, f)
ih = solve(h$Hessian)
se = diag(sqrt(ih))
MLE = M$par
Results = cbind(p, MLE, se)
> Results
     p      MLE        se
[1,] 2 2.105752 0.2105755
```
در هر برآوردیابی ابتدا واریانس را بدست می اوریم و بعد مانده ها را تحلیل میکنیم.
مانده ها به ما میگوید داده ها ناهنجاری دارد یا خیر. ص74

حدود باقی مانده ها اگر خارج از -3 و +3 باشد یعنی مشکلی در جواب ها وجود دارد.

وقتی باقی مانده ها تغییرات زیاد داشته باشد نشان می دهد جواب ها از روایی و پایایی برخوردار نیستند.

الگوریتم فلمینگ هالت به ما می گوید آیا جواب ها با هم همخوانی دارد یا خیر.


## فرضیات مدل
بررسی نرمالیتی(نمودار-ضریب چولگی-چارک ها-میانگین و میانه و مد دور و بر هم باشند)

همگونی واریانس

بررسی همبستگی(پیرسون-اسپیرمن-بی سریال)
پیرسون: پیوسته - پیوسته
بی سریال: پیوسته-کیفی
اسپیرمین: ترتیبی-ترتیبی و پیئسته-ترتیبی

بررسی هم خطی(VIF: variance inflation function)


#
ابتدا ارتباط متغیرها را با یکدیگر بررسی میکنیم. نمیتوانیم به طور قطعی تصمیم بگیریم چون داریم از RDA استفاده میکنیم و آمار استنباطی نیست.

برای بررسی نموداری ارتباط از نمودار نقطه ای(پیوسته-پوسته) و باکس پلات (پیوسته-کیفی) استفاده میکنیم. روش غیر نموداری بررسی خود واریانس هاست اگر واریانس ها دور و بر هم نباشند ناهمگونی واریانس ها داری و این خوب نیست. از نمودار باقی مانده ها نیز میتوان همگونی واریانس ها را تشخیص داد برای داده های پیوسته.

نمودار باکس پلات همگونی واریانس ها را نیز نشان میدهد. (باید همگونی واریانس داشته باشیم اگر ندشته باشیم روش های برآوردیابی مثل BLUE و ... کارایی ندارد.)

اگر همه متغیرها پیوسته باشند از نمودار پراکندگی استفاده میکنیم.

میتوان هم خطی را با رکش نموداری نیز بررسی کرد. 

از VIF میتوان هم خطی را نیز تشخیص داد.
اگر VIF کوچکتر از 2 باشد هم خطی نداریم.بین 2 تا 5 هم خطی خفیف داریم. از 10 به بالا هم خطی شدید داریم.

برای بررسی نرمالیتی به روش نموداری از نمودار هیستوگرام یا qq-plot یا pp-plot استفاده میکنیم.

روش غیر نموداری بررسی نرمالیتی را میتوان با توجه به  میانگین و مد و میانه فهمید. اگر نرمال باشد این اعداد نزدیک به هم هستند.

اگر -log(L) کم شود یعنی مدل خوب است.

معیار AIC=-2log(L) - 2p هر چه کمتر باشد مدل بهتر است.